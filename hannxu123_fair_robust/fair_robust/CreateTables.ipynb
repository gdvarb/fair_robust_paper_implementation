{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OniIYCCDuodn",
        "outputId": "cd5b25b9-4ac9-4d7f-a2f6-92502ab576c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics saved to: FRL_Replicated_Metrics_From_Code.csv\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def compute_frl_metrics(file_path):\n",
        "    \"\"\"\n",
        "    Compute Avg/Worst class-wise clean and boundary errors and robust error.\n",
        "    Only considers the last epoch row from the given result .txt file.\n",
        "    \"\"\"\n",
        "    data = np.loadtxt(file_path)\n",
        "\n",
        "    class_clean_errors = data[:, 2:12]\n",
        "    class_bndy_errors = data[:, 12:]\n",
        "    total_bndy_error = data[:, 1]  # Total robust error = clean + boundary\n",
        "\n",
        "    avg_std = np.mean(class_clean_errors[-1]) * 100\n",
        "    worst_std = np.max(class_clean_errors[-1]) * 100\n",
        "    avg_bndy = np.mean(class_bndy_errors[-1]) * 100\n",
        "    worst_bndy = np.max(class_bndy_errors[-1]) * 100\n",
        "    avg_rob = total_bndy_error[-1] * 100\n",
        "    worst_rob = worst_bndy  # Worst-case robust = worst-case boundary\n",
        "\n",
        "    return {\n",
        "        \"Avg. Std.\": round(avg_std, 1),\n",
        "        \"Worst Std.\": round(worst_std, 1),\n",
        "        \"Avg. Bndy.\": round(avg_bndy, 1),\n",
        "        \"Worst Bndy.\": round(worst_bndy, 1),\n",
        "        \"Avg. Rob.\": round(avg_rob, 1),\n",
        "        \"Worst Rob.\": round(worst_rob, 1)\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Update these paths to point to your result files\n",
        "    file_reweight = \"Report_frlrw_test_PreResNet18_100_0.050.1_0.1_0.1_0.5.txt\"\n",
        "    file_remargin = \"Report2_frlrm_test_PreResNet18_100_0.050.2_0.07_0.07_0.5.txt\"\n",
        "    file_remargin_reweight = \"Report2_frlrmrw_test_PreResNet18_100_0.050.2_0.07_0.07_0.5.txt\"\n",
        "\n",
        "    results = {\n",
        "        \"FRL(Reweight, 0.05)\": compute_frl_metrics(file_reweight),\n",
        "        \"FRL(Remargin, 0.05)\": compute_frl_metrics(file_remargin),\n",
        "        \"FRL(Reweight+Remargin, 0.05)\": compute_frl_metrics(file_remargin_reweight),\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame.from_dict(results, orient='index').reset_index()\n",
        "    df.rename(columns={'index': 'Method'}, inplace=True)\n",
        "\n",
        "    # Save to CSV\n",
        "    output_file = \"FRL_Replicated_Metrics_From_Code.csv\"\n",
        "    df.to_csv(output_file, index=False)\n",
        "    print(f\"Metrics saved to: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ]
}